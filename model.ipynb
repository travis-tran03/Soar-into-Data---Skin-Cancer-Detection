{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "a3b6be72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16912a19",
   "metadata": {},
   "source": [
    "### Get Data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "9aac1634",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_image = pd.read_csv('label_and_path.csv')\n",
    "X_image = df_image[:770]['image_path'].values\n",
    "\n",
    "df_meta = pd.read_csv('skin_data.csv')\n",
    "X_meta = df_meta[:770].drop(columns=['target']).values\n",
    "\n",
    "y = df_image[:770]['target'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd048dbb",
   "metadata": {},
   "source": [
    "### Split Data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "975f681a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split for images\n",
    "X_train_image, X_val_test_image, y_train_image, y_val_test_image = train_test_split(X_image, y, test_size=0.3, stratify=y)\n",
    "X_val_image, X_test_image, y_val_image, y_test_image = train_test_split(X_val_test_image, y_val_test_image, test_size=0.5, stratify=y_val_test_image)\n",
    "\n",
    "# Split for metadata\n",
    "X_train_meta, X_val_test_meta, y_train_meta, y_val_test_meta = train_test_split(X_meta, y, test_size=0.3, stratify=y)\n",
    "X_val_meta, X_test_meta, y_val_meta, y_test_meta = train_test_split(X_val_test_meta, y_val_test_meta, test_size=0.5, stratify=y_val_test_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "db05a085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels that are 1 in y_train_image: 267\n",
      "Number of labels that are 1 in y_val_image: 57\n",
      "Number of labels that are 1 in y_test_image: 57\n",
      "Number of labels that are 1 in y_train_meta: 267\n",
      "Number of labels that are 1 in y_val_meta: 57\n",
      "Number of labels that are 1 in y_test_meta: 57\n"
     ]
    }
   ],
   "source": [
    "count_y_train_image = np.sum(y_train_image == 1)\n",
    "count_y_val_image = np.sum(y_val_image == 1)\n",
    "count_y_test_image = np.sum(y_test_image == 1)\n",
    "\n",
    "count_y_train_meta = np.sum(y_train_meta == 1)\n",
    "count_y_val_meta = np.sum(y_val_meta == 1)\n",
    "count_y_test_meta = np.sum(y_test_meta == 1)\n",
    "\n",
    "print(f\"Number of labels that are 1 in y_train_image: {count_y_train_image}\")\n",
    "print(f\"Number of labels that are 1 in y_val_image: {count_y_val_image}\")\n",
    "print(f\"Number of labels that are 1 in y_test_image: {count_y_test_image}\")\n",
    "\n",
    "print(f\"Number of labels that are 1 in y_train_meta: {count_y_train_meta}\")\n",
    "print(f\"Number of labels that are 1 in y_val_meta: {count_y_val_meta}\")\n",
    "print(f\"Number of labels that are 1 in y_test_meta: {count_y_test_meta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "0fbc30e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_combined(image_path, metadata, label):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, (128, 128))\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    return (image, metadata), label\n",
    "\n",
    "def create_combined_dataset(image_paths, metadata, labels, batch_size=32, shuffle=True):\n",
    "    image_paths = tf.constant(image_paths)\n",
    "    metadata = tf.convert_to_tensor(metadata, dtype=tf.float32)\n",
    "    labels = tf.convert_to_tensor(labels, dtype=tf.float32)\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices((image_paths, metadata, labels))\n",
    "    ds = ds.map(preprocess_combined, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=1000)\n",
    "\n",
    "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "fc020661",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = create_combined_dataset(X_train_image, X_train_meta, y_train_image, batch_size=256)\n",
    "val_ds = create_combined_dataset(X_val_image, X_val_meta, y_val_image, batch_size=256, shuffle=False)\n",
    "test_ds = create_combined_dataset(X_test_image, X_test_meta, y_test_image, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d2e796",
   "metadata": {},
   "source": [
    "### CNN + MLP ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "f37d8209",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CNN Model\n",
    "def CNN_model(input_shape=(128, 128, 3)):\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = True  # Allow fine-tuning of the base model\n",
    "    \n",
    "    x = MaxPooling2D()(base_model.output)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    return Model(inputs=base_model.input, outputs=x, name='cnn_model')\n",
    "\n",
    "\n",
    "# MLP Model\n",
    "def MLP_model(input_dim):\n",
    "    inputs = layers.Input(shape=(input_dim,))\n",
    "    x = layers.Dense(64, activation='relu')(inputs)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(df_meta.shape[1], activation='relu')(x)\n",
    "    \n",
    "    return models.Model(inputs=inputs, outputs=x, name='metadata_model')\n",
    "\n",
    "\n",
    "# Combined Model\n",
    "def build_combined_model(image_shape, metadata_dim):\n",
    "    cnn_model = CNN_model(image_shape)\n",
    "    mlp_model = MLP_model(metadata_dim)\n",
    "\n",
    "    # Flatten the CNN output to make it 2D\n",
    "    cnn_flattened = layers.Flatten()(cnn_model.output)\n",
    "\n",
    "    # Fusion\n",
    "    combined = layers.concatenate([cnn_flattened, mlp_model.output])\n",
    "    x = layers.Dense(64, activation='relu')(combined)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    output = layers.Dense(1, activation='sigmoid')(x)  # Connect the output layer to `x`\n",
    "\n",
    "    model = models.Model(inputs=[cnn_model.input, mlp_model.input], outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "6e281a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 15s/step - accuracy: 0.4841 - loss: 12.4879 - val_accuracy: 0.4957 - val_loss: 44.7415\n",
      "Epoch 2/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 12s/step - accuracy: 0.6064 - loss: 7.7157 - val_accuracy: 0.4957 - val_loss: 17.1843\n",
      "Epoch 3/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 11s/step - accuracy: 0.7429 - loss: 6.3127 - val_accuracy: 0.4957 - val_loss: 35.3970\n",
      "Epoch 4/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 12s/step - accuracy: 0.7308 - loss: 3.8255 - val_accuracy: 0.4957 - val_loss: 29.4501\n",
      "Epoch 5/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 11s/step - accuracy: 0.7515 - loss: 3.4253 - val_accuracy: 0.4957 - val_loss: 77.6686\n",
      "Epoch 6/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 11s/step - accuracy: 0.7649 - loss: 2.4555 - val_accuracy: 0.4957 - val_loss: 96.1558\n",
      "Epoch 7/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 11s/step - accuracy: 0.8056 - loss: 1.7173 - val_accuracy: 0.4957 - val_loss: 241.7265\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2819d499c90>"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_combined_model(image_shape=(128, 128, 3), metadata_dim=X_train_meta.shape[1])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Early stopping\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=50,\n",
    "    callbacks=[early_stopping_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "e6bbfc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('combined_model2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52f84aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "f621af47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 19s/step\n"
     ]
    }
   ],
   "source": [
    "test_probs = model.predict(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "bedfd9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_probs = np.array(test_probs).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "1e989015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.        1.        1.        1.        1.        1.        1.\n",
      " 1.        1.        1.        1.        1.        1.        1.\n",
      " 1.        1.        1.        1.        1.        1.        1.\n",
      " 1.        1.        1.        1.        1.        1.        1.\n",
      " 0.9999939 1.        1.        1.        1.        1.        1.\n",
      " 1.        1.        1.        1.        1.        1.        1.\n",
      " 1.        1.        1.        1.        1.        1.        1.\n",
      " 1.        1.        1.        1.        1.        1.        1.\n",
      " 1.        1.        1.        1.        1.        1.        1.\n",
      " 1.        1.        1.        1.        1.        1.        1.\n",
      " 1.        1.        1.        1.        1.        1.        1.\n",
      " 1.        1.        1.        1.        1.        1.        1.\n",
      " 1.        1.        1.        1.        1.        1.        1.\n",
      " 1.        1.        1.        1.        1.        1.        1.\n",
      " 1.        1.        1.        1.        1.        1.        1.\n",
      " 1.        1.        1.        1.        1.        1.        1.\n",
      " 1.        1.        1.        1.       ]\n"
     ]
    }
   ],
   "source": [
    "print(test_probs[test_probs > 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "6a0f8f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.4914 - loss: 17.8571\n",
      "Test Loss: 17.857051849365234\n",
      "Test Accuracy: 0.4913793206214905\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "test_loss, test_accuracy = model.evaluate(test_ds)\n",
    "\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "41a0171b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x2819c866ef0>"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Assuming test_probs contains the predicted probabilities, we convert them to binary predictions\n",
    "y_pred = (test_probs > 0.5).astype(int)\n",
    "\n",
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(y_test_image, y_pred)\n",
    "\n",
    "# Display the confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\n",
    "disp.plot(cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "8a98d42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set - Labels with 1: 267, Labels with 0: 272\n",
      "Validation set - Labels with 1: 57, Labels with 0: 58\n",
      "Test set - Labels with 1: 57, Labels with 0: 59\n"
     ]
    }
   ],
   "source": [
    "# Count labels with 1 and 2 in each set\n",
    "count_y_train_image_1 = np.sum(y_train_image == 1)\n",
    "count_y_train_image_2 = np.sum(y_train_image == 0)\n",
    "\n",
    "count_y_val_image_1 = np.sum(y_val_image == 1)\n",
    "count_y_val_image_2 = np.sum(y_val_image == 0)\n",
    "\n",
    "count_y_test_image_1 = np.sum(y_test_image == 1)\n",
    "count_y_test_image_2 = np.sum(y_test_image == 0)\n",
    "\n",
    "print(f\"Train set - Labels with 1: {count_y_train_image_1}, Labels with 0: {count_y_train_image_2}\")\n",
    "print(f\"Validation set - Labels with 1: {count_y_val_image_1}, Labels with 0: {count_y_val_image_2}\")\n",
    "print(f\"Test set - Labels with 1: {count_y_test_image_1}, Labels with 0: {count_y_test_image_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fe85d6",
   "metadata": {},
   "source": [
    "### Auto Encoder ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b85fa46",
   "metadata": {},
   "source": [
    "# Define the convolutional autoencoder architecture\n",
    "input_shape = X_train.shape[1:]\n",
    "\n",
    "# Input layer\n",
    "input_layer = Input(shape=input_shape)\n",
    "\n",
    "# Encoding layers\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_layer)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
    "encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "# Decoding layers\n",
    "x = Conv2D(16, (3, 3), activation='relu', padding='same')(encoded)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(input_shape[2], (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "# Autoencoder model\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "\n",
    "# Encoder model\n",
    "encoder = Model(input_layer, encoded)\n",
    "\n",
    "# Compile the autoencoder\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Early stopping\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the autoencoder\n",
    "autoencoder.fit(X_train, X_train, epochs=1, batch_size=256, shuffle=True, validation_data=(X_val, X_val), callback=[early_stopping_callback])\n",
    "\n",
    "# Encode the validation data\n",
    "encoded_val = encoder.predict(X_val)\n",
    "\n",
    "# Detect anomalies using reconstruction loss\n",
    "reconstructed_val = autoencoder.predict(X_val)\n",
    "reconstruction_error = np.mean((X_val - reconstructed_val) ** 2, axis=(1, 2, 3))\n",
    "\n",
    "# Set a threshold for anomaly detection\n",
    "threshold = reconstruction_error.mean() + 2 * reconstruction_error.std()\n",
    "\n",
    "# Identify anomalies\n",
    "anomalies = reconstruction_error > threshold\n",
    "print(\"Number of anomalies detected:\", anomalies.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d61fe7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs178",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
